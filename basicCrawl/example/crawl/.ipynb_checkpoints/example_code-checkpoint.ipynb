{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Course Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Environment\n",
    "`pip install requests`  \n",
    "`pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 hello crawl (Get Method)\n",
    "\n",
    "url: https://www.ntust.edu.tw/home.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# use get method to request  \n",
    "response = requests.get('https://www.ntust.edu.tw/home.php')\n",
    "\n",
    "# change encoding to uft-8\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "print('status code is: {}'.format(response.status_code))\n",
    "print('response html is: \\n {}'.format(response.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 hello crawl (POST Method)\n",
    "url: https://www.vscinemas.com.tw/ShowTimes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "payload = {\n",
    "\"CinemaCode\":\"QS\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#https://www.vscinemas.com.tw/ShowTimes//ShowTimes/GetShowTimes\n",
    "response = requests.post(\"https://www.vscinemas.com.tw/ShowTimes//ShowTimes/GetShowTimes\",data = payload)\n",
    "print(response.text)\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#print(len(soup.find_all(\"strong\",{\"class\":\"MovieName\"}))//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "payload = {\n",
    "    \"stkNo\":\"2330\"\n",
    "}\n",
    "response = requests.post(\"https://www.twse.com.tw/zh/stockSearch/stockSearch\", data = payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Beautifulsoup Example\n",
    "url: https://isaac60103.github.io/crawl_course/example/beautifulsoup_example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get(\"https://isaac60103.github.io/crawl_course/example/beautifulsoup_example.html\")\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "# use beautifulsoup to analyze html\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #response.text代表把response用text型態表示。\n",
    "\n",
    "\n",
    "print('find the first h1 tag')\n",
    "print(soup.find(\"h1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('show the first h1 tag content')\n",
    "print(soup.find(\"h1\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find the first td tag')\n",
    "print(soup.find(\"td\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('show the first td tag content')\n",
    "print(soup.find(\"td\").text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find all td tag')\n",
    "print(soup.find_all(\"td\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find all tag with class = group1')\n",
    "print(soup.find_all(attrs={\"class\":\"group1\"})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('show the first div tag with id=id1')\n",
    "print(soup.find(\"div\", {\"id\":\"id1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find all tag with text= hello_python')\n",
    "print(soup.find_all(text = \"hello_python\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find third td tag')\n",
    "print(soup.find_all(\"td\")[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find a tag within third td tag')\n",
    "print(soup.find_all(\"td\")[2].find(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find the first a tag and return href')\n",
    "print(soup.find(\"a\")['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(\"div\")['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4 Prettify HTML Code via Beautifulsoup\n",
    "url: http://edition.cnn.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('http://edition.cnn.com/')\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.prettify()) # prettify把HTML的一行代碼，分行顯示，容易閱讀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "payload = {'SearchType': 'S', 'Lang':'TW', 'StartStation':'TaiPei', 'EndStation':'ZuoYing',\n",
    "           'OutWardSearchDate':'2022/05/26', 'OutWardSearchTime':'09:30', 'ReturnSearchDate':'2022/05/26',\n",
    "           'ReturnSearchTime':'23:00', 'DiscountType':''\n",
    "          }\n",
    "response = requests.post('https://www.thsrc.com.tw/TimeTable/Search', data = payload)\n",
    "# print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(response.text)\n",
    "# print(result)\n",
    "for i in result ['data']['DepartureTable']['TrainItem']:\n",
    "    print('TrainNumber:{}, DepartureTime:{}, DestinationTime:{}'.format(i['TrainNumber'], i['DepartureTime'], i['DestinationTime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-5 Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "pattern='abc'\n",
    "string='123321 abc abc abc'\n",
    "match = re.findall(pattern,string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"a+b*c\"\n",
    "string = 'aabc, ac, aabbccc abb, dd'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"a*b+c\"\n",
    "string = 'abbbbc, bc, bcccc,c, acc'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#pattern = \"[0-9]+\"\n",
    "\n",
    "pattern = \"[0123456789]+\"\n",
    "\n",
    "string = 'there are 77 apples, 110 bananas, and 5 pineapples'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[1-3]+\"\n",
    "string = 'abc123, aaabbcc111123, aaa123333bbccc, 1222223'\n",
    "match =re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[dor]mit\"\n",
    "string = 'admit, commit, emit, omit, permit'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"isa{2,5}c\"\n",
    "string = 'These are my friends kevin, andy, isaac, isac, isaaaac, isaaac,and maggie'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[A-Z]+[a-z]\"\n",
    "string = 'ABi, BBc, CNn, ai, be, cd'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"\\..{2}\"\n",
    "string = 'www.yahoo.com.tw , www.ntu.edu.tw , www.test.gov.tw'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"I have a dream|I don't have to work\"\n",
    "string = 'I have a dream that I don\\'t have to work'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(([A-Za-z0-9._]+)@[A-Za-z.]+com|edu)\"\n",
    "string = 'isaac60103@gmail.com, isaac60103@hotmail.com, kevin@yahoo.com'\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "response = requests.get('https://isaac60103.github.io/crawl_course/practice/email_list.html')\n",
    "string = response.text\n",
    "pattern = \"([A-Za-z0-9._]+@[A-Za-z.]+[com|edu])\"\n",
    "match = re.findall(pattern, string)\n",
    "print(match)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-6 Beautifulsoup with Regular Expression\n",
    "url: https://isaac60103.github.io/crawl_course/example/beautifulsoup_example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://isaac60103.github.io/crawl_course/example/beautifulsoup_example.html')\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# get all tags with class = groupXXX \n",
    "all_tds = soup.find_all(attrs= {'class':re.compile('group[0-9]')})\n",
    "\n",
    "for td in all_tds:\n",
    "    print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://zh.wikipedia.org/api/rest_v1/page/summary/%E6%8A%80%E8%A1%93%E5%9E%8B%E9%AB%98%E7%B4%9A%E4%B8%AD%E7%AD%89%E5%AD%B8%E6%A0%A1\")\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "allTds = soup.find_all(attrs= {'td', text = re.compile('信義')})\n",
    "\n",
    "print(allTds)\n",
    "\n",
    "# for td in all_tds:\n",
    "#     if td == '信義':\n",
    "#         print(td)\n",
    "https://www.thsrc.com.tw/Event/Logo.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-7 Crawl Images or Files\n",
    "url: https://www.thsrc.com.tw/tw/TimeTable/SearchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "url = 'https://www.thsrc.com.tw/tw/TimeTable/SearchResult'\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "all_imgs = soup.find_all('img')\n",
    "\n",
    "for index, img in enumerate(all_imgs):\n",
    "    image_path = 'https://www.thsrc.com.tw'+img['src']\n",
    "    image_path = image_path.replace(' ', '')\n",
    "    image_name = img['src'].split('/')[-1]\n",
    "    print('image path is {} , file name is {}'.format(image_path, image_name))\n",
    "    urlretrieve(image_path , 'save_image/'+image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"https://www.go100.com.tw/file/exam/S110-GSAT/110GSAT_Chinese.pdf\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "all_files = soup.find_all('file')\n",
    "\n",
    "for index, img in enumerate(all_files)\n",
    "    files_path = 'https://www.go100.com.tw/'+file['src']\n",
    "    files_name = files['src'].split('/')[-1]\n",
    "    print(files.path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4\n",
      "0   亞洲股市指數行情  亞洲股市指數行情  亞洲股市指數行情  亞洲股市指數行情  亞洲股市指數行情\n",
      "1         股市        指數        漲跌        比例        當地\n",
      "2        紐西蘭  11173.37    -73.66    -0.66%     18:00\n",
      "3       澳洲股市   7413.00     39.80     0.54%     16:31\n",
      "4      日經225  26739.00     -9.14    -0.03%     14:31\n",
      "5       東證一部   1880.75      2.49     0.13%     14:11\n",
      "6       東證二部   7228.27    -10.79    -0.15%     04/01\n",
      "7     JASDAQ    164.28     -0.47    -0.29%     04/01\n",
      "8       韓國股市   2624.24     18.37     0.70%     14:31\n",
      "9       台灣加權  16142.09    178.46     1.12%     13:29\n",
      "10      台灣店頭    194.80      2.22     1.15%     13:12\n",
      "11      上海綜合   3092.21     21.28     0.69%     13:31\n",
      "12      上海A股   3241.13     23.01     0.72%     13:31\n",
      "13      上海B股    301.17      3.02     1.01%     13:15\n",
      "14      深圳A股   2027.44     16.22     0.81%     13:08\n",
      "15      深圳B股   1081.12     -0.33    -0.03%     13:08\n",
      "16     滬深300   3966.34      7.19     0.18%     13:31\n",
      "17      深證成指  11106.16     40.24     0.36%     13:01\n",
      "18      中小板指   7630.95     32.54     0.43%     13:17\n",
      "19      創業板指   2325.48      7.41     0.32%     13:01\n",
      "20      香港恆生  20166.00     53.90     0.27%     13:31\n",
      "21      香港國企   6899.39     16.25     0.24%     13:16\n",
      "22      香港紅籌   3841.69      8.90     0.23%     13:25\n",
      "23      恆生科技   4177.65   -106.77    -2.49%     05/23\n",
      "24     香港創業板     37.34     -0.19    -0.51%     13:28\n",
      "25       新加坡   3184.71    -10.33    -0.32%     13:25\n",
      "26       菲律賓   6603.17     25.72     0.39%     13:16\n",
      "27      馬來西亞   1534.36      3.06     0.20%     13:16\n",
      "28      越南股市   1247.84     14.46     1.17%     11:32\n",
      "29      泰國股市   1636.02      9.79     0.60%     12:16\n",
      "30      印尼股市   6883.42    -30.72    -0.44%     11:30\n",
      "31      印度股市  54108.63     56.02     0.10%     10:46\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'http://www.stockq.org/'\n",
    "table_lists = pd.read_html(url)\n",
    "#動態頁面無法使用，僅限於靜態頁面，就乖乖寫爬蟲。\n",
    "print(table_lists[12])\n",
    "df = table_lists[12]\n",
    "df.to_csv('stock_result.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-8 User Agent\n",
    "url: http://www.ntu.edu.tw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# add user agent here to cheat web server\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:57.0) Gecko/20100101 Firefox/57.0'}\n",
    "response = requests.get('http://www.ntu.edu.tw/', headers=headers)\n",
    "#response = requests.get('http://www.ntu.edu.tw/')\n",
    "\n",
    "response.encoding = 'utf-8'\n",
    "print(response.text)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-9 Captcha Solver\n",
    "reference web: https://2captcha.com/   \n",
    "installation guide: https://github.com/Mirio/captcha2upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captcha2upload import CaptchaUpload\n",
    "\n",
    "my_key = ''\n",
    "file_path = './captcha_demo.jpeg'\n",
    "\n",
    "captcha = CaptchaUpload(my_key)\n",
    "print(captcha.solve(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-10 Retry and Exception with Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "url = 'https://isaac60103.github.io/crawl_course/example/beautifulsoup_example.html'\n",
    "\n",
    "max_retry = 3\n",
    "retry = 1\n",
    "\n",
    "while retry <= max_retry:\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        sleep(2)\n",
    "        \n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        all_tds = soup.find_all(\"td\")\n",
    "\n",
    "        for td in all_tds:\n",
    "            print(td)\n",
    "        \n",
    "        break\n",
    "    except Exception as e:\n",
    "        retry += 1\n",
    "        print(\"error: {}\".format(e))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-11 Crawl when webpage update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import hashlib\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.ntu.edu.tw/'\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:57.0) Gecko/20100101 Firefox/57.0'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.encoding = 'utf-8'\n",
    "sig = hashlib.md5(response.text.encode('utf-8')).hexdigest()\n",
    "old_sig=''\n",
    "\n",
    "if os.path.exists('eq_sig.txt'):\n",
    "    with open('eq_sig.txt', 'r') as fp:\n",
    "        old_sig = fp.read()\n",
    "    with open('eq_sig.txt', 'w') as fp:\n",
    "        fp.write(sig)\n",
    "else:\n",
    "    with open('eq_sig.txt', 'w') as fp:\n",
    "        fp.write(sig)\n",
    "\n",
    "if sig == old_sig:\n",
    "    print('data not update, don\\'t need parse data')\n",
    "    exit()\n",
    "else:\n",
    "    print('data update! Start analysis data')\n",
    "    \n",
    "    '''\n",
    "    use beautifulsoup or regular expression to analysis data ... ....\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-12 DB Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('test.db')\n",
    "\n",
    "first_number = 20 \n",
    "second_number = 10\n",
    "\n",
    "sqlstr = \"insert into test_table (field1, field2) values({},{});\".format(first_number,second_number)\n",
    "conn.execute(sqlstr)\n",
    "conn.commit()\n",
    "\n",
    "sqlstr = \"select * FROM test_table;\"\n",
    "cursor = conn.execute(sqlstr)\n",
    "for row in cursor:\n",
    "    print('{} {} {}'.format(row[0], row[1], row[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-13 DB with Pandas\n",
    "url: http://www.taipeibo.com/year/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "response = requests.get('http://www.taipeibo.com/year/2017')\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "# convert html table into dataframe\n",
    "# note that return type is a list\n",
    "df_list = pd.read_html(response.text)\n",
    "df = df_list[0]\n",
    "print(df)\n",
    "\n",
    "# connect to db. if db is not exist, it will create a new one\n",
    "conn = sql.connect(\"movie_review.db\")\n",
    "\n",
    "# insert dataframe into table. If table exist, it would replace it\n",
    "df.to_sql(\"moview_review_table\", conn, if_exists=\"replace\")\n",
    "\n",
    "# query from DB \n",
    "df_query = pd.read_sql_query(\"select * from moview_review_table;\", conn).head()\n",
    "print(df_query.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEJ API\n",
    "ref:https://api.tej.com.tw/documents.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tejapi\n",
    "tejapi.ApiConfig.api_key = ''\n",
    "data = tejapi.get('TWN/APRCD',coid='Y9999')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_list = pd.read_html('https://www.rottentomatoes.com/critics/latest_reviews/')\n",
    "df = df_list[0]\n",
    "df.head()\n",
    "df.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_list = pd.read_html('http://www.stockq.org/')\n",
    "df = df_list[9]\n",
    "print(df.head())\n",
    "df.to_csv('stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
